\section{Discussion}\label{sec:Discussion}


\subsection{Regression on generated data}
The method of ordinary least squares gives the smallest estimation of the error, when predicting on the data generated by the Franke function. When studying the models behaviour for different complexities, we see from figure \ref{fig:ols_frankie_train_test_bias_variance} (left) that the predictions on the test set has a minimum mean squared error $\text{MSE} = 0.0136$, which occurs at degree $d = 5$. The predictions on the training set keep declining for higher complexities, while the predictions on the test set start to rise. When models of complexity $d > 5$ are introduced to data which it has not seen before, it fails to reproduce the testing data reliably. This suggests that the model has been fitted too closely to the training data and captured elements of noise, rather than the trends in the data itself.

Having a look at figure \ref{fig:ols_frankie_train_test_bias_variance} (right) again, we see that in the area $1 \leq d < 5$ the error in the prediction consists solely of the bias term, while the variance remains almost non-existent. This is most likely due to the fact that the data cannot be captured accurately by a model with such a low complexity. The predictions by the model are so off target, relatively speaking, that the differences in the training and test data are mostly irrelevant compared to the model and the trends in the data. For $d > 5$, the variance term begins increasing, while the bias declines. This is most likely due to the model being, again, over fitted, and too sensitive to small fluctuations in the data set. Ideally we want to choose a model which is complex enough to capture the regularities in the training data, but also generalized well to data which it has not seen. As seen, this middle ground is struck with a polynomial of degree $d=5$.

From \ref{tab:franke} we see that Ridge regression performs almost equally as well as OLS. With the optimal parameter $\log_{10}(\lambda) = -4$ and polynomial degree $d = 6$, the estimated error in the prediction is $\text{MSE} = 0.0139$. Having a look at the tuning of the parameters $\lambda$ and $d$ in figure \ref{fig:ridge_heatmap_frankie}, we see that its performance is generally good for small values of $\lambda$, which means that it is generally an OLS estimate, and may suggest that there is little correlation between the estimators in the model. When looking back at \ref{fig:ridge_frankie_train_test_bias_variance}, where we did a separate analysis of the performance of Ridge with the optimal value for $\lambda$, we can see that we have the same problem with overfitting when the complexity of the model is higher than $d = 6$. In this plot, the minimum MSE occurs at $d = 6$, which is different from the analysis in \ref{fig:ridge_heatmap_frankie}, even though the models had the same parameters. A possible explanation for this is the random selection of data done by the Bootstrap resampling. We have chosen a relatively small number of 100 Bootstrap resamples, which may have resulted in a fluctuation favourable to one of the configurations. In retrospect, we could have looked at the histogram of the mean values of the randomly selected values, to ensure that it was normal.

When performing Lasso regression on the generated data, we see from the heat map \ref{fig:lasso_heatmap} that the optimal model parameters which give the lowest prediction of the error $\text{MSE} = 0.0183$, are $\log_{10}(\lambda) = -8$ and complexity $d = 5$. We also see from the figure that for values of $\lambda < 1$, the model does a very bad job of reproducing the data. One reason for this may be that some of the coefficients $\beta$ have been penalized too much to be able to reproduce the data in any meaningful way. This was also true for Ridge, but even more so for Lasso, which makes sense considering that the penalization parameter in Lasso regression is capable of shrinking the $\beta$ parameters all the way to zero for greater values of $\lambda$. We have also had a closer look at the performance of the Lasso model in figure \ref{fig:lasso_test_train_bias_variance} with $\log_{10}(\lambda) = -8$, and plotted the MSE of the prediction for both the testing and training data. We see that the MSE of the test data has several local minima, which is quite interesting, and a global minimum at $d=5$ with $\text{MSE}=0.0221$. The trend of the training error keeps declining for Lasso as well, which is what we expect to see. The bias-variance decomposition of the test data error in the same figure (\ref{fig:lasso_test_train_bias_variance}) shows that the error is largely composed of the bias$^2$ term, and that the variance in the prediction is very low in comparison. In general, when comparing the Lasso method to OLS and Ridge, our results suggest that Lasso is a less suitable choice when trying to represent our data with a model. 


\subsection{Regression on real terrain data}

Figure \ref{fig:bias_variance_all_methods} shows the error analysis of the terrain data, with all three different types of regression methods. The OLS and Ridge regression gives very similar values for both the MSE, bias$^2$ and variance, which may indicate that the shrinkage imposed by the hyper parameter $\lambda$ does not contribute much to the overall result when using Ridge regression. In fact, as we can see from table \ref{tab:terrain}, the minimum MSE obtained with the OLS and Ridge method is 0.0091 and 0.0092 respectively. The OLS regression gives slightly better results, if we only use MSE as a measure of the quality of fit, but not by much. The third plot in figure \ref{fig:bias_variance_all_methods} shows the error analysis for the Lasso methods, and it shows a similar trend of the three quantities (MSE, bias$^2$ and variance) as we have seen with the OLS and Ridge regression. The minimum MSE in this case is 0.0138 for a polynomial degree $d = 10$, and clearly performs worse than the OLS and Ridge regression. All three plots show a variance that is constantly very close to zero, which means that it contributes very little to the overall error, while bias, on the other hand, is shown to be the main source of error in the models. 

Because of limitations in computational resources, we have only tested the models up to a polynomial degree of 10, but the error analysis in figure \ref{fig:bias_variance_all_methods} suggests that for all methods, the error will decrease further for higher degrees, given the declining trend of the MSE. Since our models generally have a very low variance and a relatively high bias$^2$, it seems like the models are too simple to give a good representation of the terrain.

The confidence intervals of the parameters $\beta_j$, which is presented in figure \ref{fig:OLS_terrain_confidence_intervals}, shows that the parameters with greater values generally have larger confidence intervals, which is what we would expect. The coefficients with index $j<10$ have very small values. The $\beta_j$-values with the lowest and highest indices are very small compared to the rest, and indicate that the predictors they represent do not affect the model much. As mentioned above, the bias-variance plots for the terrain model suggest that fitting with an even higher polynomial degree may have resulted in a reduced error, but in the plot of the confidence intervals of $\beta_j$ it seems like the highest order coefficients are relatively unimportant.

In figure \ref{fig:model_terrain} we see the real terrain data plotted next to our best-performing (in terms of having the lowest MSE) model, which was the OLS method with degree $d=10$. A visual inspection tells us that the model seems to be following the main features of the terrain. The edges of the surfaces are easiest to inspect, and they match fairly well. Even so, the actual topographical data is very jagged, and our model gives an oversimplified and too smooth representation of the terrain.

In all of our analysis of the methods, the models have been fitted to the training data, but since the model configurations we have found to be best in each case is based on the predictions on the test data, the reported best model configurations is by extension fitted to the test data. It may therefore be wise to in stead consider what we have called the test data in this report as validation data, and have a separate set of test data which we finally predict our ideal model on, to give a less biased estimation of the models error.