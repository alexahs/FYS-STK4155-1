\section{Methods}\label{sec:Methods}
\subsection{Formatting the data}\label{sec:data_format}
We have two sources of data, one is generated by the Frankie function (equation \ref{eq:franke}), and the other is real terrain data in the form of elevation maps. In both cases, the prediction parameters are the $x_1$ and $x_2$ coordinates of the domain, and the height $y$ is the output. For the Frankie data, we generate $n$ uniformly distributed values of $x_1, x_2 \in [0,1]$ and place them in a mesh grid.  We evaluate the function at each point in the grid, and add stochastic noise $\epsilon \in \mathcal{N}(0, \sigma^2)$ to each point.

In the case of the real terrain data\footnote{The original data file can be found at the website \url{https://earthexplorer.usgs.gov} with Entity ID: SRTM1N41W001V3}, this is simply imported as a \textit{.tif} file, giving the terrain height in meters, which is rescaled to be in units of kilometers. We also resize the image, such that only $1/16$th of the original image is used. The original image is of dimensions $3600 \times 1800$ organized in a 2D array, where we have select the $900$ first points in the $x_1$ and $x_2$ directions as our data set. Finally we slice every other point in this array, such that the final image is of dimension $450 \times 450$. This is done due to memory issues when operating on larger data sets.

We now initialize our design matrix by organizing the values of $x_1$ and $x_2$ according to equation \ref{eq:designmatrix}, such that each column consist of a unique combination of powers $x_1^i x_2^{p-i}$, $i = 0,1,\dots,p$, and each row containing unique combinations of $x_1$ and $x_2$ values. We now have $\boldsymbol{X} \in \mathbb{R}^{n^2 \times p}$. The $\boldsymbol{y}$ values in our data are flattened to a 1D array, resulting in our target vector $\boldsymbol{y} \in \mathbb{R}^{n^2 \times 1}$. 

Ideally one would want to normalize the data, which involves subtracting the mean of each column from the corresponding column in $\boldsymbol{X}$ and also dividing each column by its standard deviation. However, we have had varying results when trying to implement this, but have not viewed it as a big problem, as the values of the data generated by the Franke function is mostly in the region $y \in [0, 1]$, which is also the case of the terrain data after converting from meters to kilometers.

\subsection{Regression Methods}
The main class in our implementation which performs the regression methods is called \textit{RegressionMethods}, and is instantiated simply with a specification of the desired method, that is, either ordinary least squares, Ridge or Lasso. To train the model, the class has a method called \textit{fit}, which takes as arguments the training data $\boldsymbol{X}_{train}$, and calls upon the relevant regression method. The ordinary least squares method finds the optimal $\boldsymbol{\beta}^{OLS}$ values by equation \ref{eq:betaoptols}, and does this by using the Numpy library's matrix multiplication functionality and \textit{linalg.pinv} function for finding the inverse matrix\footnote{In cases where the inverse matrix does not exists, it instead computes the pseudo-inverse.}. In the same manner, it computes the optimal $\boldsymbol{\beta}^{Ridge}$ by solving equation \ref{eq:betaoptridge}. In this case, the class must also be instantiated with the desired penalty parameter $\lambda$. This is also the case when using the class' Ridge regression method. We have not implemented this method ourselves, but instead uses the functionality of Scikit Learn's \textit{Lasso} function. Any instance of the model can also predict a set of input parameters different from those it has been trained on, this is done by calling the \textit{predict} method with an input parameter $\boldsymbol{X}_{test}$, which calculates the matrix-vector multiplication of $\boldsymbol{X}_{test}\boldsymbol{\beta} = \boldsymbol{\Tilde{y}}$.

\subsection{Resampling}
Our resampling method of choice is Bootstrapping, which we have implemented such that it first splits our data into training and test data, then randomly selects points and fits the model to these selected points. When this process has been completed a desired number of times, it calculates the error metrics of the predictions on both the test and training data. Pseudo-code for the algorithm:
\begin{algorithm}[!h]
{Split the data $\boldsymbol{X}$ and $\boldsymbol{y}$ into training data and test data}\\
\For{$n_{bootstraps}$}{
    {Select $n_{train}$ random samples from the training data}\\
    {Train the model on $\boldsymbol{X}_{train}$}\\
    {Test the model on $\boldsymbol{X}_{test}$}\\
    {Save all predicted values $\boldsymbol{\Tilde{y}}$}\\
}
{Calculate mean MSE, $R^2$ score, bias$^2$ and variance}
\end{algorithm}

In order to find the optimal parameters which gives the best fit to our data, we have also implemented a function which performs the above algorithm for chosen model value ranges of $\lambda$ and polynomial degrees $d$, and uses the MSE as a measure for how well the particular model performs.

\subsection{Benchmarks}
We have constructed a test case, which tests the results of our implementations of OLS and Ridge against the predictions of Scikit learn's methods. The data which is used in the test is generated from the Franke function, with $100 \times 100$ points. We then calculate the mean squared error between the prediction on this data produced by our code and Scikit learn's. In both cases the difference in the predictions was to machine presicion.
\subsection{Source code}

The source code of this project is written in Python, and can be found in the GitHub repository at \url{https://github.com/alexahs/FYS-STK4155/tree/master/Project_1}. The repository consists of the following files:
\begin{itemize}
    \item RegressionMethods.py
    \item Resamplnig.py
    \item unitTests.py
    \item functions.py
    \item analysis.py
    \item main.py
\end{itemize}


